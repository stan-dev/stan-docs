# Gaussian Processes  {#gaussian-processes.chapter}

Gaussian processes are continuous stochastic processes and thus may be
interpreted as providing a probability distribution over functions.  A
probability distribution over continuous functions may be viewed,
roughly, as an uncountably infinite collection of random variables,
one for each valid input.  The generality of the supported functions
makes Gaussian priors popular choices for priors in general
multivariate (non-linear) regression problems.

The defining feature of a Gaussian process is that the joint distribution of
the function's value at a finite number of input points is a multivariate
normal distribution.  This makes it tractable to both fit models from finite
amounts of observed data and make predictions for finitely many new data
points.

Unlike a simple multivariate normal distribution, which is
parameterized by a mean vector and covariance matrix, a Gaussian
process is parameterized by a mean function and covariance function.
The mean and covariance functions apply to vectors of inputs and
return a mean vector and covariance matrix which provide the mean and
covariance of the outputs corresponding to those input points in the
functions drawn from the process.

Gaussian processes can be encoded in Stan by implementing their mean and
covariance functions and plugging the result into the Gaussian form of their
sampling distribution, or by using the specialized covariance functions
outlined below.  This form of model is straightforward and may be used for
simulation, model fitting, or posterior predictive inference. A more efficient
Stan implementation for the GP with a normally distributed outcome marginalizes
over the latent Gaussian process, and applies a Cholesky-factor
reparameterization of the Gaussian to compute the likelihood and the posterior
predictive distribution analytically.

After defining Gaussian processes, this chapter covers the basic
implementations for simulation, hyperparameter estimation, and
posterior predictive inference for univariate regressions,
multivariate regressions, and multivariate logistic regressions.
Gaussian processes are  general, and by necessity this chapter
only touches on some basic models.  For more information, see
@RasmussenWilliams:2006.


## Gaussian Process Regression

The data for a multivariate Gaussian process regression consists of a
series of $N$ inputs $x_1,\dotsc,x_N \in \mathbb{R}^D$ paired with outputs
$y_1,\dotsc,y_N \in \mathbb{R}$.  The defining feature of Gaussian
processes is that the probability of a finite number of outputs $y$
conditioned on their inputs $x$ is Gaussian:
$$
y \sim \textsf{multivariate normal}(m(x), K(x \mid \theta)),
$$
where $m(x)$ is an $N$-vector and $K(x \mid \theta)$ is an $N \times N$
covariance matrix.  The mean function $m : \mathbb{R}^{N \times D}
\rightarrow \mathbb{R}^{N}$ can be anything, but the covariance function
$K : \mathbb{R}^{N \times D} \rightarrow \mathbb{R}^{N \times N}$ must produce
a positive-definite matrix for any input $x$.^[Gaussian processes can be extended to covariance functions producing positive semi-definite matrices, but Stan does not support inference in the resulting models because the resulting distribution does not have unconstrained support.]

A popular covariance function, is an exponentiated quadratic function, also known as a radial basis function, or a gaussian kernel, and will be used for implementations later in this chapter.
$$
  K(x, x' \mid \tau, l)
= \tau^2
\exp \left(
- \dfrac{||x - x'||_2^2}{2 l^2} \right)
$$
where $\tau$ and $l$ are hyperparameters defining the
covariance function. $x$ and $x'$ are different rows, or observations of the input data matrix, or design matrix. 

We add $\sigma^2$ on the diagonal
to ensure the positive definiteness of the resulting matrix in the case of
two identical inputs $x_i = x_j$, for the stability of cholesky factorization.
In statistical terms, $\sigma$ is the scale of the noise term in the regression.

The hyperparameter $l$ is the *length-scale*, and corresponds to the
frequency of the functions represented by the Gaussian process prior with
respect to the domain. Values of $l$ closer to zero lead the GP to represent
high-frequency functions, whereas larger values of $l$ lead to low-frequency
functions. The hyperparameter $\tau$ is the *marginal standard
deviation*. It controls the magnitude of the range of the function represented
by the GP. If you were to take the standard deviation of many draws from the GP
$f_1$ prior at a single input $x$ conditional on one value of $\tau$ one
would recover $\tau$.

The only term in the squared exponential covariance function involving
the inputs $x_i$ and $x_j$ is their vector difference, $x_i - x_j$.
This produces a process with stationary covariance in the sense that
if an input vector $x$ is translated by a vector $\epsilon$ to $x +
\epsilon$, the covariance at any pair of outputs is unchanged, because
$K(x \mid \theta) = K(x + \epsilon \mid \theta)$.

The summation involved is just the squared Euclidean distance between
$x_i$ and $x_j$ (i.e., the $L_2$ norm of their difference, $x_i -
x_j$). This results in support for smooth functions in the process.
The amount of variation in the function is controlled by the free
hyperparameters $\tau$, $l$, and $\sigma$.

Changing the notion of distance from Euclidean to taxicab distance
(i.e., an $L_1$ norm) changes the support to functions which are
continuous but not smooth.

## Simulating from a Gaussian Process

We first start with a Stan model that simulates
draws of functions $f$ from a Gaussian process. The model
implemented below will draw values $y_n = f(x_n)$ for finitely many
input points $x_n$.

This Stan model first defines the mean and covariance functions in a
transformed data block. Then, it samples outputs $y$ in the model using
a multivariate normal distribution. we will use the squared exponential
covariance function with hyperparameters set to $\tau^2 = 1$, $l^2 = 1$,
and $\sigma^2 = 0.001$. We set the mean function as the zero vector, $m(x) = \textbf{0}$,
meaning the functions simulated will always be mean centered.
In Stan, we can simulate from a Gaussian Process as follows

```
data {
  int<lower=1> N;
  real x[N];
}
transformed data {
  matrix[N, N] K;
  vector[N] mu = rep_vector(0, N);
  for (i in 1:N) {
    for (j in i:N) {
      K[i, j] = exp(-0.5 * square(x[i] - x[j]));
      K[j, i] = K[i, j];
    }
  }
  K = add_diag(K, .001);
}
parameters {
  vector[N] y;
}
model {
  y ~ multi_normal(mu, K);
}
```

We can write this model more concisely using a built-in
function that implements the exponantial quadratic kernel.

```
data {
  int<lower=1> N;
  real x[N];
}
transformed data {
  matrix[N, N] K = gp_exp_quad_cov(x, 1.0, 1.0);
  vector[N] mu = rep_vector(0, N);
  K = add_diag(K, .001);
}
parameters {
  vector[N] y;
}
model {
  y ~ multi_normal(mu, K);
}
```

The input data are the vector of inputs `x` and its size
`N`. To vizualize these functions, we can compute `x` as an evenly spaced
grid over some interval. For example, an evenly spaced grid in 'R' from 0
to 100 would be: `seq(0, 100, length.out = 100)`, and in python, using the
numpy library as `np`, this would be: `np.linspace(0, 100, 100)`.


### Inputs with Multiple Predictors {-}

To change from a single predictor model to one with multiple predictors, we need to change
the input data. The lines that need to change are

```
data {
  int<lower=1> N;
  int<lower=1> D;
  vector[D] x[N];
}
transformed data {
...
```

Before, the data were declared as an array of scalars, that is, a design
matrix with a single predictor. Now, we declare the input data as an array
of vectors, to mean an $N \times D$ design matrix, with $D$ predictors.

In the remainder of the chapter, we will use single predictor models for simplicity,
but any of the models can be changed to have multiple predictors in the same way as the
sampling model. The only extra computational overhead from a
multiple predictor model is in the distance calculation.

### Cholesky Factored and Transformed Implementation {-}

A more efficient implementation of the simulation model above can be
coded in Stan by relocating, rescaling and rotating an isotropic standard
normal variate.  Let $\eta_n$ be distributed standard normally,
$$
\eta_n \sim \textsf{normal}(\textbf{0}_n, I_n),
$$
where $\textbf{0}$ is an $N$-vector of 0 values and $I_n$ is the 
$N\times N$ identity matrix.  Let $L$ be the Cholesky factorization of
$K(x \mid \theta)$. That is, $L$ is the lower-triangular matrix $L$ such that
$LL^{\top} = K(x \mid \theta)$. Then $\mu + L\eta$ is distributed multivariate
normally, with mean $\mu(x)$ and covariance $K(x \mid \theta)$

$$
  \mu + L\eta \sim \textsf{multivariate normal}(\mu(x), K(x \mid \theta)).
$$

We can use this transformation to simulate from a Gaussian Process.

This model has the same data declarations for `N` and `x`,
and the same transformed data definitions of `mu` and
`K` as the previous model, with the addition of a transformed
data variable for the Cholesky decomposition.  The parameters change
to the raw parameters sampled from an isotropic standard normal, and the
actual samples are defined as generated quantities.

```
...
transformed data {
  matrix[N, N] L;
...
  L = cholesky_decompose(K);
}
parameters {
  vector[N] eta;
}
model {
  eta ~ std_normal();
}
generated quantities {
  vector[N] y;
  y = mu + L * eta;
}
```

The Cholesky decomposition is only computed once after the data are
loaded and the covariance matrix `K` computed.  The isotropic
normal distribution for `eta` is specified as a vectorized
univariate distribution for efficiency; this specifies that each
`eta[n]` has an independent standard normal distribution.  The sampled
vector `y` is then defined as a generated quantity using a direct
encoding of the transform described above.

## Fitting a Gaussian Process {#fit-gp.section}

### GP with a normal outcome {-}

The full generative model for a GP with a normal outcome,
$y \in \mathbb{R}^N$, with inputs $x \in \mathbb{R}^N$, for a finite $N$:
\begin{align*}
l   &\sim \textsf{InvGamma}(5, 5) \\
\tau &\sim \textsf{normal}(0, 1) \\
\sigma &\sim \textsf{normal}(0, 1) \\
f      &\sim \textsf{multivariate normal}\left(0, K(x \mid \alpha, \rho)\right) \\
y_i    &\sim \textsf{normal}(f_i, \sigma) \, \forall i \in \{1, \dots, N\}
\end{align*}
With a normal outcome, it is possible to integrate out the Gaussian
process $f$, yielding the more parsimonious model:
\begin{align*}
l   &\sim \textsf{InvGamma}(5, 5) \\
\tau &\sim \textsf{normal}(0, 1) \\
\sigma &\sim \textsf{normal}(0, 1) \\
y      &\sim \textsf{multivariate normal}
         \left(0, K(x \mid \alpha, \rho) + \textbf{I}_N \sigma^2\right) \\
\end{align*}

It can be more computationally efficient when dealing with a normal
outcome to integrate out the Gaussian process, because this yields a
lower-dimensional parameter space over which to integrate. We'll fit
both models in Stan. The former model will be referred to as the latent
variable GP, while the latter will be called the marginal likelihood
GP.

The hyperparameters controlling the covariance function of a Gaussian
process can be fit by assigning them priors, like we have in the
generative models above, and then computing the posterior distribution
of the hyperparameters given observed data. The priors on the
parameters should be defined based on prior knowledge of the scale of
the output values ($\alpha$), the scale of the output noise
($\sigma$), and the scale at which distances are measured among inputs
($\rho$). See the [Gaussian process priors section](#priors-gp.section)
for more information about how to specify
appropriate priors for the hyperparameters.

The Stan program implementing the marginal likelihood GP is shown below. The
program is similar to the Stan programs that implement the simulation GPs
above, but because we are doing inference on the hyperparameters, we need to
calculate the covariance matrix `K` in the model block, rather than
the transformed data block.

```
data {
  int<lower=1> D;
  int<lower=1> N;
  real x[N];
  vector[N] y;

  int<lower=1> N_pred;
  real x_pred[N_pred];
}
parameters {
  real<lower=0> tau;
  real<lower=0> l;
  real<lower=0> sigma;
  
  real<lower=0> sigma;
}
transformed parameters {
  matrix[N, N] L_K;
  {
    matrix[N, N] K = gp_exp_quad_cov(x, magnitude, length_scale);
    K = add_diag(K, square(sigma));
    L_K = cholesky_decompose(K);
  }
}
model {
  magnitude ~ normal(0, 1);
  length_scale ~ gamma(5, 5);
  sig ~ normal(0, 1);

  y ~ multi_normal_cholesky(mu, L_K);
}
```

The data block declares a vector `y` of observed values `y[n]`
for inputs `x[n]`.  The transformed data block defines the mean
vector to be zero.  We define three non-negative hyper parameters.
The computation of the covariance matrix `K` is now in the model
block because it involves unknown parameters and can't be precomputed
as transformed data.  The rest of the model
consists of the priors for the hyperparameters and the multivariate
Cholesky-parameterized normal likelihood. The value `y` is known
and the covariance matrix `K` is an unknown dependent on the
hyperparameters, allowing us to estimate the hyperparameters.

We have used the Cholesky parameterized multivariate normal rather
than the standard parameterization because it allows us to the
`cholesky_decompose` function. This function is optimized for both
small and large matrices. For larger matrices ($N \gtrsim 100$) the Cholesky
decomposition version will be faster, but speed differences are neglible for
small matrices.

Hamiltonian Monte Carlo sampling is fast and effective for hyperparameter
inference in this model [@Neal:1997]. If the posterior is
well-concentrated for the hyperparameters the Stan implementation will fit
hyperparameters in non-Gaussian Process models with a few hundred data points in seconds.

#### Latent variable GP {-}

Here, we will show how to implement the latent variable formulation of a GP in Stan.
This is particularly useful for when the likelihood is non-gaussian.


```
data {
  int<lower=1> N;
  real x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real<lower=0> l;
  real<lower=0> tau;
  real<lower=0> sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = gp_exp_quad_cov(x, tau, l);
    K = add_diag(K, delta);

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }

  l ~ inv_gamma(5, 5);
  tau ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}
```


There are two differences between the latent variable GP and the marginal likelihood
GP worth noting. First, we have augmented our parameter block with
a new parameter vector of length $N$ called `eta`. This is used in the model
block to generate a multivariate normal vector called $f$, corresponding to the
latent GP. We put a $\textsf{normal}(0,1)$ prior on `eta` like we did in the
Cholesky-parameterized GP in the simulation section.  Second, our likelihood
is now univariate, though we could code $N$ likelihood terms as one
$N$-dimensional multivariate normal with an identity covariance
matrix multiplied by $\sigma^2$. However, it is more efficient to use the
vectorized statement as shown above.

### Discrete outcomes with Gaussian Processes {-}

Gaussian processes can be generalized the same way as linear
models by introducing a link function.  This allows them to be used as
discrete data models, in other likelihood functions, or as a prior on a
parameter.

#### Poisson GP {-}

If we want to model count data, we can remove the $\sigma$ parameter, and use
`poisson_log`, which implements a log link. We can also add an overall
mean parameter, $a$, which accounts for the marginal expected value
for $y$. This is because we cannot center count data like we would
for normally distributed data.


```
data {
...
  int<lower=0> y[N];
...
}
...
parameters {
  real<lower=0> l;
  real<lower=0> tau;
  real a;
  vector[N] eta;
}
model {
...
  l ~ inv_gamma(5, 5);
  tau ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  y ~ poisson_log(a + f);
}
```


#### Logistic Gaussian Process Regression {-}

For binary classification problems, the observed outputs $z_n \in
\{ 0,1 \}$ are binary.  These outputs are modeled using a Gaussian
process with (unobserved) outputs $y_n$ through the logistic link,
$$
z_n \sim \textsf{Bernoulli}(\operatorname{logit}^{-1}(y_n))
$$
or equivalently
$$
\textsf{Pr}[z_n = 1] = \operatorname{logit}^{-1}(y_n)
$$

We can extend our latent variable GP Stan program to deal with classification
problems. Below `a` is the bias term, which can help account for imbalanced
classes in the training data:


```
data {
...
  int<lower=0, upper=1> z[N];
...
}
...
model {
...

  y ~ bernoulli_logit(a + f);
}
```


### Automatic Relevance Determination {-}

If we have multiple inputs $x \in \mathbb{R}^D$, we can fit a scale
parameter $\mathbf{l} = \{l_1, l_2, ..., l_d\}$ for each dimension $d$,

$$
  k(x \mid \tau, \mathbf{l}, \sigma)_{i, j} = \tau^2 \exp
\left(-\dfrac{1}{2}
\sum_{d=1}^D \dfrac{1}{l_d^2} ||x_{i,d} - x_{j,d}||^2
\right)
$$
The estimation of $\rho$ was termed "automatic relevance determination" by
@Neal:1996, but this is misleading because the magnitude of the scale of
the posterior for each $l_d$ is dependent on the scale of the input data
along dimension $d$. Moreover, the scale of the parameters $l_d$ measures
non-linearity along the $d$-th dimension, rather than "relevance"
[@PiironenVehtari:2016].

A priori, the closer $l$ is to zero, the more nonlinear the
conditional mean in dimension $d$ is.  A posteriori, the actual dependencies
between $x$ and $y$ play a role.  With one covariate $x_1$ having a
linear effect and another covariate $x_2$ having a nonlinear effect,
it is possible that $l_1 > l_2$ even if the predictive relevance
of $x_1$ is higher [@RasmussenWilliams:2006, page 80].
The collection of $l_d$ (or $1/l_d$) parameters can also be
modeled hierarchically.

We can implement this by first dividing each dimension of the data matrix $\mathbb{x}$
by the corresponding length-scale parameter, and then computing the squared distance, taking
the exponential, and multiplying by the magnitude parameter. In Stan code, this is


```
functions {
  matrix L_gp_exp_quad_cov_ARD(vector[] x,
                            real alpha,
                            vector rho,
                            real delta) {
    int N = size(x);
    matrix[N, N] K;
    real sq_alpha = square(alpha);
    for (i in 1:(N-1)) {
      K[i, i] = sq_alpha + delta;
      for (j in (i + 1):N) {
        K[i, j] = sq_alpha
                      * exp(-0.5 * dot_self((x[i] - x[j]) ./ rho));
        K[j, i] = K[i, j];
      }
    }
    K[N, N] = sq_alpha + delta;
    return cholesky_decompose(K);
  }
}
data {
  int<lower=1> N;
  int<lower=1> D;
  vector[D] x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  vector<lower=0>[D] rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K = L_cov_exp_quad_ARD(x, alpha, rho, delta);
    f = L_K * eta;
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}
```

Equivalently, we can use a call to `gp_exp_quad_cov` but instead specifying the length-scale parameter as a vector of positive real numbers


```
data {
  int<lower=1> N;
  int<lower=1> D;
  vector[D] x[N];
  vector[N] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  vector<lower=0>[D] l;
  real<lower=0> tau;
  real<lower=0> sigma;
  vector[N] eta;
}
model {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = gp_exp_quad_cov(x, tau, l);
    K = add_diag(K, square(delta));
    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }

  l ~ inv_gamma(5, 5);
  tau ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f, sigma);
}
```

### Priors for Gaussian Process Parameters {#priors-gp.section}

Specifying priors for GP hyperparameters requires the analyst to consider the
GP's purpose in the model and the numerical issues that may arise in Stan when
estimating a GP.

In particular, the parameters $l$ and $\tau$ are weakly
identified [@zhang-gp:2004]. The ratio of the two
parameters is well-identified, but in practice we put independent priors on the
two hyperparameters because these two quantities are more interpretable than
their ratio.

#### Priors for length-scale {-}

GPs can represent a wide spectrum of functions.  For length scales less than the minimum
distance of the covariates, the GP likelihood plateaus.  Unless regularized by a prior, this flat
likelihood induces considerable posterior mass at small length scales where the
observation variance drops to zero and the functions supported by the GP being
to exactly interpolate between the input data.  The resulting posterior not
only significantly overfits to the input data, it also becomes hard to
accurately sample using Euclidean HMC.

We may wish to put further soft constraints on the length-scale, but these are
dependent on how the GP is used in our statistical model.

If our model is a GP regression, i.e.
\begin{align*}
  y_i &= f(x_i) + \epsilon_i \\
  \epsilon_i &\sim \textsf{normal}(0, \sigma) \\
 f &\mid x; \tau, l \sim \textsf{multivariate normal}\left(0, K(x \mid \tau, l)\right) \\
  x &\in \mathbb{R}^{N \times D}, \quad \sigma, \tau, l \in \mathbb{R}^+, 
  \quad i \in \{1, \dots, N\}
\end{align*}

we might not need constraints beyond penalizing small
length-scales.  If we'd like to allow the GP prior to represent both
high-frequency and low-frequency functions, then we can set our prior to 
put non-negligible mass on both sets of functions.  If this is the case, an
inverse gamma, `inv_gamma_lpdf` in Stan's language, will work well.
This is because it has a sharp left tail that puts negligible mass on infinitesimally
small length-scales, but has a diffuse right tail, allowing for large
length-scales. The posterior will be pushed away from zero since 
the density is zero at zero. An inverse gamma distribution is one
of many zero-avoiding or boundary-avoiding distributions.^[A boundary-avoiding
prior is where the limit of the prior density is zero at the boundary,
so that posterior estimates are pushed away from the boundary.].

Suppose for example we're using the GP as a submodel of a larger model that 
includes a linear model for the same covariates we're using as the input for the
GP. That is,

\begin{align*}
y_i &= \beta_0 + x_i \beta + f(x_i) + \epsilon_i \\
\epsilon_i &\sim \textsf{normal}(0, \sigma) \\
f   &\sim \textsf{multivariate normal}\big(0, K(x \mid \tau, l)\big) \\
    & \beta_0 \in \mathbb{R},\quad
      \beta \in \mathbb{R}^D,\quad\\
      x &\in \mathbb{R}^{N \times D},\quad
      \sigma, \tau, l \in \mathbb{R}^+, 
  \quad i \in \{1, \dots, N\}
      
\end{align*}

we might want to constrain large length-scales as well.  A length scale
that is larger than the scale of the data yields a GP posterior that is
practically linear (with respect to the particular covariate). Increasing
the length scale has little impact on the likelihood. This will introduce
nonidentifiability in our model since both the fixed effects and the GP will
explain similar variation. In order to limit the amount of overlap between the
GP and the linear regression, we can use a prior with a sharper right tail
to limit the GP to higher-frequency functions. We can use a generalized inverse
Gaussian distribution:

\begin{align*}
\pi(x \mid a, b, p) &= \dfrac{\left(a/b\right)^{p/2}}{2K_p\left(\sqrt{ab}\right)} x^{p - 1}\exp\big(-(ax + b
  / x)/2\big) \\
  & x, a, b \in \mathbb{R}^{+},\quad
    p \in \mathbb{Z}
\end{align*}

which has an inverse gamma left tail if $p \leq 0$ and an inverse Gaussian
right tail.  This can be implemented as a user-defined function:
```
functions {
  real generalized_inverse_gaussian_lpdf(real x, int p,
                                        real a, real b) {
    return p * 0.5 * log(a / b)
      - log(2 * modified_bessel_second_kind(p, sqrt(a * b)))
      + (p - 1) * log(x)
      - (a * x + b / x) * 0.5;
 }
}
data {
...
```

If we have high-frequency covariates in our linear model, we may want to 
regularize the GP away from high-frequency functions. We can achieve this
by penalizing smaller length-scales. Luckily, we have a useful way of
thinking about how length-scale affects the frequency of the functions
supported by the GP. If we were to repeatedly draw from a zero-mean GP with a
length-scale of $l$ in a fixed-domain $[0,T]$, we would get a distribution
for the number of times each draw of the GP crossed the zero axis. The
expectation of this random variable, the number of zero crossings, is $\frac{T}{\pi
l}$. You can see that as $l$ decreases, the expectation of the number of
upcrossings increases as the GP is representing higher-frequency functions.
Thus, this is a good statistic to keep in mind when setting a lower-bound for
our prior on length-scale in the presence of high-frequency covariates.
However, this statistic is only valid for one-dimensional inputs.

#### Priors for marginal standard deviation {-}

The parameter $\tau$ corresponds to how much of the variation is
explained by the regression function. This parameter has a similar 
role to the prior variance for linear model weights.  This means
we can use the same prior for $\tau$ as we use in linear models,
such as a half-$t$ prior on $\tau$.

A half-$t$ or half-Gaussian prior on $\tau$ also has the benefit of putting
more prior mass around zero. This allows the GP to support zero
functions and allows the possibility that the GP won't contribute to the
conditional mean of the total output.

### Predictive Inference with a Gaussian Process {-}

Suppose for a given sequence of inputs $x$ that the corresponding
outputs $y$ are observed.  Given a new sequence of inputs $\tilde{x}$,
the posterior predictive distribution of their labels is computed by
sampling outputs $\tilde{y}$ according to
$$
p\left(\tilde{y} \mid \tilde{x},x,y\right)
\ = \
\frac{p\left(\tilde{y}, y \mid \tilde{x},x\right)}
     {p(y \mid x)}
\ \propto \
p\left(\tilde{y}, y \mid \tilde{x},x\right).
$$

A direct implementation in Stan defines a model in terms of the
joint distribution of the observed $y$ and unobserved $\tilde{y}$.


```
data {
  int<lower=1> N;
  real x[N];
  vector[N] y;
  
  int<lower=1> N_pred;
  real x_pred[N_pred];
}
transformed data {
  real delta = 1e-9;
  int<lower=1> N_tot = N + N_pred;
  real x[N_tot];
  for (n in 1:N) x[n] = x[n];
  for (n_pred in 1:N_pred) x[N + n_pred] = x_pred[n_pred];
}
parameters {
  real<lower=0> tau;
  real<lower=0> l;
  real<lower=0> sigma;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = gp_exp_quad_cov(x, tau, l);

    K = add_diag(K, square(delta));

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  l ~ inv_gamma(5, 5);
  tau ~ std_normal();
  sigma ~ std_normal();
  eta ~ std_normal();

  y ~ normal(f[1:N], sigma);
}
generated quantities {
  vector[N_pred] y_pred;
  for (n_pred in 1:N_pred)
    y_pred[n_pred] = normal_rng(f[N + n_pred], sigma);
}
```

Equivalently, we can use the posterior predictive mean and variance 
equations for Gaussian process regression:


\begin{align*}
E[\tilde{f}] &= K_{\tilde{x}, x} (K_{x,x}  + \delta_n^2 I)^{-1}y \\
cov(\tilde{f}) &= K_{\tilde{x}, \tilde{x}} - K_{\tilde{x}, x} (K_{x,x}  + \delta_n^2 I)^{-1}K_{x, \tilde{x}}
\end{align*}

Here, $K_{x, x}$ is covariance matrix computed with input data and
$K_{\tilde{x},x}$ is the cross covariance matrix computed with input data 
and data we'd like to make predictions for. We implement these functions in 
Stan's `functions` block. The above implementation will be more efficient, in that
it uses less floating point operations. However, the following implementation can 
give us more flexibility when visualizing the posterior if we combine covariarance 
functions. We will investigate this in a later section.

```
functions {
  vector gp_pred_rng(vector[] x_pred,
                     vector y, vector[] x,
                     real tau, real[] l, real sigma, real delta) {
    int N = rows(y);
    int N_pred = size(x_pred);
    vector[N_pred] f_pred;
    {
      matrix[N, N] K =   gp_exp_quad_cov(x, tau, l)
                         + diag_matrix(rep_vector(square(sigma), N));
      matrix[N, N] L_K = cholesky_decompose(K);

      vector[N] L_K_div_y = mdivide_left_tri_low(L_K, y);
      vector[N] K_div_y = mdivide_right_tri_low(L_K_div_y', L_K)';
      matrix[N, N_pred] k_x_x_pred = gp_exp_quad_cov(x, x_pred, tau, l);
      vector[N_pred] f_pred_mu = (k_x_x_pred' * K_div_y);
      matrix[N, N_pred] v_pred = mdivide_left_tri_low(L_K, k_x_x_pred);
      matrix[N_pred, N_pred] cov_f_pred =   gp_exp_quad_cov(x_pred, tau, l) - v_pred' * v_pred
                              + diag_matrix(rep_vector(delta, N_pred));
      f_pred = multi_normal_rng(f_pred_mu, cov_f_pred);
    }
    return f_pred;
  }
}
data {
  int<lower=1> D;
  int<lower=1> N;
  vector[D] x[N];
  vector[N] y;

  int<lower=1> N_pred;
  vector[D] x_pred[N_pred];
}
transformed data {
  real delta = 1e-4;
}
parameters {
  real<lower=0> l[D];
  real<lower=0> tau;
  real<lower=0> sigma;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = gp_exp_quad_cov(x, tau, l);
    K = add_diag(K, square(delta));
    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  tau ~ std_normal();
  l ~ inv_gamma(5, 5);
  sigma ~ std_normal();
  
  y ~ normal(f, sigma);
}
generated quantities {
  vector[N_pred] f_pred = gp_pred_rng(x_pred, y, x, tau, l, sigma, delta);
  vector[N_pred] y_pred;
  for (n in 1:N_pred) y_pred[n] = normal_rng(f_pred[n], sigma);
}
```


#### Predictive Inference GPs with non Gaussian Response Distributions {-}

We can do predictive inference in non-Gaussian likelihoods GPs similarly 
to how we do predictive inference with Gaussian likelihoods.

Consider the following model for prediction using logistic regression with a 
Gaussian process prior on the log-odds probability that $z = 1$

\begin{align*}
f(x) &\sim GP(\mu(x), K(x)) \\
p(k \mid \alpha_0, \alpha) &\sim logit^{-1}(\alpha_0 + f)^z (1 - logit^{-1}(\alpha_0 + f))^{1-z} \\
z &\in \{0,1\}
\end{align*}

where we include an intercept term, $\alpha_0$ to improve fit.

$$

```
data {
  int<lower=1> N;
  real x[N];
  int<lower=0, upper=1> z[N];
  
  int<lower=1> N_pred;
  real x_pred[N_pred];
}
transformed data {
  real delta = 1e-4;
}
parameters {
  real<lower=0> l;
  real<lower=0> tau;
  real alpha0;
  vector[N] eta;
}
transformed parameters {
  vector[N] f;
  {
    matrix[N, N] L_K;
    matrix[N, N] K = cov_exp_quad(x, tau, l);
    K = add_diag(K, square(delta));

    L_K = cholesky_decompose(K);
    f = L_K * eta;
  }
}
model {
  l ~ inv_gamma(5, 5);
  tau ~ std_normal();
  a ~ std_normal();
  eta ~ std_normal();

  z ~ bernoulli_logit(a + f);
}
generated quantities {
  int z_pred[N2];
  for (n2 in 1:N2)
    z2[n2] = bernoulli_logit_rng(a + f;
}
```


#### Analytical Form of Joint Predictive Inference {-}

Bayesian predictive inference for Gaussian processes with Gaussian observations
can be sped up by deriving the posterior analytically, then directly sampling
from it.

Jumping straight to the result,
$$
p\left(\tilde{y} \mid \tilde{x},y,x\right)
=
\textsf{normal}\left(K^{\top}\Sigma^{-1}y,\
                \Omega - K^{\top}\Sigma^{-1}K\right),
$$
where $\Sigma = K(x \mid \alpha, \rho, \sigma)$ is the result of applying the covariance
function to the inputs $x$ with observed outputs $y$, $\Omega =
K(\tilde{x} \mid \alpha, \rho)$ is the result of applying the covariance function to the
inputs $\tilde{x}$ for which predictions are to be inferred, and $K$
is the matrix of covariances between inputs $x$ and $\tilde{x}$, which
in the case of the exponentiated quadratic covariance function
would be
$$
K(x \mid \alpha, \rho)_{i, j} = \eta^2 \exp\left(-\dfrac{1}{2 \rho^2}
\sum_{d=1}^D \left(x_{i,d} - \tilde{x}_{j,d}\right)^2\right).
$$

There is no noise term including $\sigma^2$ because the indexes of
elements in $x$ and $\tilde{x}$ are never the same.

This Stan code below uses the analytic form of the posterior and provides
sampling of the resulting multivariate normal through the Cholesky
decomposition. The data declaration is the same as for the latent variable
example, but we've defined a function called `gp_pred_rng` which will
generate a draw from the posterior predictive mean conditioned on observed data
`y1`. The code uses a Cholesky decomposition in triangular solves in order
to cut down on the number of matrix-matrix multiplications when computing
the conditional mean and the conditional covariance of $p(\tilde{y})$.

```
functions {
  vector gp_pred_rng(real[] x2,
                     vector y1,
                     real[] x1,
                     real alpha,
                     real rho,
                     real sigma,
                     real delta) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] L_K;
      vector[N1] K_div_y1;
      matrix[N1, N2] k_x1_x2;
      matrix[N1, N2] v_pred;
      vector[N2] f2_mu;
      matrix[N2, N2] cov_f2;
      matrix[N2, N2] diag_delta;
      matrix[N1, N1] K;
      K = cov_exp_quad(x1, alpha, rho);
      for (n in 1:N1)
        K[n, n] = K[n,n] + square(sigma);
      L_K = cholesky_decompose(K);
      K_div_y1 = mdivide_left_tri_low(L_K, y1);
      K_div_y1 = mdivide_right_tri_low(K_div_y1', L_K)';
      k_x1_x2 = cov_exp_quad(x1, x2, alpha, rho);
      f2_mu = (k_x1_x2' * K_div_y1);
      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      cov_f2 = cov_exp_quad(x2, alpha, rho) - v_pred' * v_pred;
      diag_delta = diag_matrix(rep_vector(delta, N2));

      f2 = multi_normal_rng(f2_mu, cov_f2 + diag_delta);
    }
    return f2;
  }
}
data {
  int<lower=1> N1;
  real x1[N1];
  vector[N1] y1;
  int<lower=1> N2;
  real x2[N2];
}
transformed data {
  vector[N1] mu = rep_vector(0, N1);
  real delta = 1e-9;
}
parameters {
  real<lower=0> rho;
  real<lower=0> alpha;
  real<lower=0> sigma;
}
model {
  matrix[N1, N1] L_K;
  {
    matrix[N1, N1] K = cov_exp_quad(x1, alpha, rho);
    real sq_sigma = square(sigma);

    // diagonal elements
    for (n1 in 1:N1)
      K[n1, n1] = K[n1, n1] + sq_sigma;

    L_K = cholesky_decompose(K);
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();

  y1 ~ multi_normal_cholesky(mu, L_K);
}
generated quantities {
  vector[N2] f2;
  vector[N2] y2;

  f2 = gp_pred_rng(x2, y1, x1, alpha, rho, sigma, delta);
  for (n2 in 1:N2)
    y2[n2] = normal_rng(f2[n2], sigma);
}
```

### Multiple-output Gaussian processes {-}

Suppose we have observations $y_i \in \mathbb{R}^M$ observed at
$x_i \in \mathbb{R}^K$. One can model the data like so:
\begin{align*}
y_i  &\sim \textsf{multivariate normal}\left(f(x_i), \textbf{I}_M \sigma^2\right) \\
f(x) &\sim \textsf{GP}\big(m(x), K(x \mid \theta, \phi)\big) \\
     & K(x \mid \theta) \in \mathbb{R}^{M \times M}, \quad
       f(x), m(x) \in \mathbb{R}^M
\end{align*}
where the $K(x, x^\prime \mid \theta, \phi)_{[m, m^\prime]}$ entry defines the
covariance between $f_m(x)$ and $f_{m^\prime}(x^\prime)(x)$. This construction
of Gaussian processes allows us to learn the covariance between the output
dimensions of $f(x)$. If we parameterize our kernel $K$:
$$
K(x, x^\prime \mid \theta, \phi)_{[m, m^\prime]} = k\left(x, x^\prime \mid
\theta\right) k\left(m, m^\prime \mid \phi\right)
$$
then our finite dimensional generative model for the above is:
\begin{align*}
f        &\sim \textsf{matrixnormal}\big(m(x), K(x \mid \alpha, \rho), C(\phi)\big) \\
y_{i, m} &\sim \textsf{normal}(f_{i,m}, \sigma) \\
f        &\in  \mathbb{R}^{N \times M}
\end{align*}
where $K(x \mid \alpha, \rho)$ is the exponentiated quadratic kernel we've used
throughout this chapter, and $C(\phi)$ is a positive-definite matrix,
parameterized by some vector $\phi$.

The matrix normal distribution has two covariance matrices: $K(x \mid
\alpha, \rho)$ to encode column covariance, and $C(\phi)$ to define row
covariance. The salient features of the matrix normal are that the rows
of the matrix $f$ are distributed:
$$
f_{[n,]} \sim \textsf{multivariate normal}\big(m(x)_{[n,]}, K(x \mid \alpha,
\rho)_{[n,n]} C(\phi)\big)
$$
and that the columns of the matrix $f$ are
distributed:
$$
f_{[,m]} \sim \textsf{multivariate normal}\big(m(x)_{[,m]}, K(x
  \mid \alpha, \rho) C(\phi)_{[m,m]}\big)
$$
This also means means that $\mathbb{E}\left[f^T f\right]$ is equal to
$\operatorname{trace}\!\big(K(x \mid \alpha, \rho)\big) \times C$, whereas $\mathbb{E}\left[ff^T\right]$
is $\operatorname{trace}(C) \times K(x \mid \alpha, \rho)$. We can derive this using
properties of expectation and the matrix normal density.

We should set $\alpha$ to $1.0$ because the parameter is not identified unless
we constrain $\operatorname{trace}(C) = 1$. Otherwise, we can multiply $\alpha$ by a scalar $d$ and
$C$ by $1/d$ and our likelihood will not change.

We can generate a random variable $f$ from a matrix normal density in
$\mathbb{R}^{N \times M}$ using the following algorithm:
\begin{align*}
\eta_{i,j} &\sim \textsf{normal}(0, 1) \, \forall i,j \\
f          &= L_{K(x \mid 1.0, \rho)} \, \eta \, L_C(\phi)^T \\
f          &\sim \textsf{matrixnormal}\big(0, K(x \mid 1.0, \rho), C(\phi)\big) \\
\eta       &\in \mathbb{R}^{N \times M} \\
L_C(\phi)  &= \texttt{cholesky}\mathtt{\_}\texttt{decompose}\big(C(\phi)\big) \\
L_{K(x \mid 1.0, \rho)} &= \texttt{cholesky}\mathtt{\_}\texttt{decompose}\big(K(x \mid 1.0, \rho)\big)
\end{align*}

This can be implemented in Stan using a latent-variable GP formulation. We've used
$\textsf{LKJCorr}$ for $C(\phi)$, but any positive-definite matrix will do.


```
data {
  int<lower=1> N;
  int<lower=1> D;
  real x[N];
  matrix[N, D] y;
}
transformed data {
  real delta = 1e-9;
}
parameters {
  real<lower=0> rho;
  vector<lower=0>[D] alpha;
  real<lower=0> sigma;
  cholesky_factor_corr[D] L_Omega;
  matrix[N, D] eta;
}
model {
  matrix[N, D] f;
  {
    matrix[N, N] K = cov_exp_quad(x, 1.0, rho);
    matrix[N, N] L_K;

    // diagonal elements
    for (n in 1:N)
      K[n, n] = K[n, n] + delta;

    L_K = cholesky_decompose(K);
    f = L_K * eta
        * diag_pre_multiply(alpha, L_Omega)';
  }

  rho ~ inv_gamma(5, 5);
  alpha ~ std_normal();
  sigma ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(3);
  to_vector(eta) ~ std_normal();

  to_vector(y) ~ normal(to_vector(f), sigma);
}
generated quantities {
  matrix[D, D] Omega;
  Omega = L_Omega * L_Omega';
}
```
